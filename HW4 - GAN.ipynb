{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=1, img_size=28, latent_dim=100, lr=0.0002, n_cpu=8, n_epochs=200, sample_interval=400)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval betwen image samples\")\n",
    "opt = parser.parse_known_args()[0]\n",
    "print(opt)\n",
    "\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs(\"../../data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../../data/mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/938] [D loss: 0.685344] [G loss: 0.692719]\n",
      "[Epoch 1/200] [Batch 0/938] [D loss: 0.408207] [G loss: 1.981441]\n",
      "[Epoch 2/200] [Batch 0/938] [D loss: 0.499612] [G loss: 2.593513]\n",
      "[Epoch 3/200] [Batch 0/938] [D loss: 0.319716] [G loss: 1.820756]\n",
      "[Epoch 4/200] [Batch 0/938] [D loss: 0.433993] [G loss: 0.654341]\n",
      "[Epoch 5/200] [Batch 0/938] [D loss: 0.306322] [G loss: 2.761988]\n",
      "[Epoch 6/200] [Batch 0/938] [D loss: 0.246787] [G loss: 1.162958]\n",
      "[Epoch 7/200] [Batch 0/938] [D loss: 0.351661] [G loss: 0.892814]\n",
      "[Epoch 8/200] [Batch 0/938] [D loss: 0.287028] [G loss: 1.463980]\n",
      "[Epoch 9/200] [Batch 0/938] [D loss: 0.158447] [G loss: 2.716348]\n",
      "[Epoch 10/200] [Batch 0/938] [D loss: 0.243302] [G loss: 2.546278]\n",
      "[Epoch 11/200] [Batch 0/938] [D loss: 0.326983] [G loss: 1.364102]\n",
      "[Epoch 12/200] [Batch 0/938] [D loss: 0.207347] [G loss: 2.389528]\n",
      "[Epoch 13/200] [Batch 0/938] [D loss: 0.268481] [G loss: 1.591209]\n",
      "[Epoch 14/200] [Batch 0/938] [D loss: 0.103179] [G loss: 2.060884]\n",
      "[Epoch 15/200] [Batch 0/938] [D loss: 0.430108] [G loss: 0.755715]\n",
      "[Epoch 16/200] [Batch 0/938] [D loss: 0.131097] [G loss: 2.733818]\n",
      "[Epoch 17/200] [Batch 0/938] [D loss: 0.245631] [G loss: 1.795838]\n",
      "[Epoch 18/200] [Batch 0/938] [D loss: 0.292688] [G loss: 3.578540]\n",
      "[Epoch 19/200] [Batch 0/938] [D loss: 0.253658] [G loss: 1.446760]\n",
      "[Epoch 20/200] [Batch 0/938] [D loss: 0.176223] [G loss: 1.867245]\n",
      "[Epoch 21/200] [Batch 0/938] [D loss: 0.176061] [G loss: 3.495968]\n",
      "[Epoch 22/200] [Batch 0/938] [D loss: 0.186443] [G loss: 2.254514]\n",
      "[Epoch 23/200] [Batch 0/938] [D loss: 0.185750] [G loss: 3.159956]\n",
      "[Epoch 24/200] [Batch 0/938] [D loss: 0.231904] [G loss: 1.969160]\n",
      "[Epoch 25/200] [Batch 0/938] [D loss: 0.137811] [G loss: 3.489311]\n",
      "[Epoch 26/200] [Batch 0/938] [D loss: 0.103350] [G loss: 3.702332]\n",
      "[Epoch 27/200] [Batch 0/938] [D loss: 0.221515] [G loss: 6.174937]\n",
      "[Epoch 28/200] [Batch 0/938] [D loss: 0.097923] [G loss: 3.385885]\n",
      "[Epoch 29/200] [Batch 0/938] [D loss: 0.151474] [G loss: 2.895535]\n",
      "[Epoch 30/200] [Batch 0/938] [D loss: 0.125173] [G loss: 3.474912]\n",
      "[Epoch 31/200] [Batch 0/938] [D loss: 0.084682] [G loss: 3.664891]\n",
      "[Epoch 32/200] [Batch 0/938] [D loss: 0.773520] [G loss: 8.510841]\n",
      "[Epoch 33/200] [Batch 0/938] [D loss: 0.069681] [G loss: 3.620547]\n",
      "[Epoch 34/200] [Batch 0/938] [D loss: 0.057853] [G loss: 3.267705]\n",
      "[Epoch 35/200] [Batch 0/938] [D loss: 0.133926] [G loss: 2.987831]\n",
      "[Epoch 36/200] [Batch 0/938] [D loss: 0.614416] [G loss: 7.958813]\n",
      "[Epoch 37/200] [Batch 0/938] [D loss: 0.141699] [G loss: 3.499405]\n",
      "[Epoch 38/200] [Batch 0/938] [D loss: 0.097874] [G loss: 2.977262]\n",
      "[Epoch 39/200] [Batch 0/938] [D loss: 0.140818] [G loss: 4.054350]\n",
      "[Epoch 40/200] [Batch 0/938] [D loss: 0.145316] [G loss: 3.848266]\n",
      "[Epoch 41/200] [Batch 0/938] [D loss: 0.084316] [G loss: 3.374455]\n",
      "[Epoch 42/200] [Batch 0/938] [D loss: 0.086264] [G loss: 3.368989]\n",
      "[Epoch 43/200] [Batch 0/938] [D loss: 0.155574] [G loss: 2.312702]\n",
      "[Epoch 44/200] [Batch 0/938] [D loss: 0.235915] [G loss: 3.281053]\n",
      "[Epoch 45/200] [Batch 0/938] [D loss: 0.078382] [G loss: 3.273052]\n",
      "[Epoch 46/200] [Batch 0/938] [D loss: 0.149952] [G loss: 1.908446]\n",
      "[Epoch 47/200] [Batch 0/938] [D loss: 0.167028] [G loss: 1.864300]\n",
      "[Epoch 48/200] [Batch 0/938] [D loss: 0.104140] [G loss: 3.500440]\n",
      "[Epoch 49/200] [Batch 0/938] [D loss: 0.154736] [G loss: 2.851105]\n",
      "[Epoch 50/200] [Batch 0/938] [D loss: 0.219306] [G loss: 1.891933]\n",
      "[Epoch 51/200] [Batch 0/938] [D loss: 0.094338] [G loss: 2.699342]\n",
      "[Epoch 52/200] [Batch 0/938] [D loss: 0.115727] [G loss: 3.479709]\n",
      "[Epoch 53/200] [Batch 0/938] [D loss: 0.222034] [G loss: 6.185619]\n",
      "[Epoch 54/200] [Batch 0/938] [D loss: 0.138565] [G loss: 2.138039]\n",
      "[Epoch 55/200] [Batch 0/938] [D loss: 0.075976] [G loss: 3.330667]\n",
      "[Epoch 56/200] [Batch 0/938] [D loss: 0.096064] [G loss: 3.036804]\n",
      "[Epoch 57/200] [Batch 0/938] [D loss: 0.021516] [G loss: 3.941782]\n",
      "[Epoch 58/200] [Batch 0/938] [D loss: 0.138094] [G loss: 2.465319]\n",
      "[Epoch 59/200] [Batch 0/938] [D loss: 0.140674] [G loss: 5.715162]\n",
      "[Epoch 60/200] [Batch 0/938] [D loss: 0.092258] [G loss: 3.321714]\n",
      "[Epoch 61/200] [Batch 0/938] [D loss: 0.139892] [G loss: 2.574937]\n",
      "[Epoch 62/200] [Batch 0/938] [D loss: 0.296771] [G loss: 1.461092]\n",
      "[Epoch 63/200] [Batch 0/938] [D loss: 0.285494] [G loss: 3.276361]\n",
      "[Epoch 64/200] [Batch 0/938] [D loss: 0.207750] [G loss: 2.245714]\n",
      "[Epoch 65/200] [Batch 0/938] [D loss: 0.209340] [G loss: 1.939443]\n",
      "[Epoch 66/200] [Batch 0/938] [D loss: 0.138713] [G loss: 4.551259]\n",
      "[Epoch 67/200] [Batch 0/938] [D loss: 0.180556] [G loss: 5.060066]\n",
      "[Epoch 68/200] [Batch 0/938] [D loss: 0.132760] [G loss: 3.145582]\n",
      "[Epoch 69/200] [Batch 0/938] [D loss: 0.239844] [G loss: 2.568733]\n",
      "[Epoch 70/200] [Batch 0/938] [D loss: 0.325264] [G loss: 4.730046]\n",
      "[Epoch 71/200] [Batch 0/938] [D loss: 0.289984] [G loss: 1.554930]\n",
      "[Epoch 72/200] [Batch 0/938] [D loss: 0.158360] [G loss: 2.246975]\n",
      "[Epoch 73/200] [Batch 0/938] [D loss: 0.133374] [G loss: 2.838373]\n",
      "[Epoch 74/200] [Batch 0/938] [D loss: 0.145964] [G loss: 2.138528]\n",
      "[Epoch 75/200] [Batch 0/938] [D loss: 0.212417] [G loss: 2.290942]\n",
      "[Epoch 76/200] [Batch 0/938] [D loss: 0.169780] [G loss: 2.591847]\n",
      "[Epoch 77/200] [Batch 0/938] [D loss: 0.139865] [G loss: 2.801158]\n",
      "[Epoch 78/200] [Batch 0/938] [D loss: 0.196174] [G loss: 2.202194]\n",
      "[Epoch 79/200] [Batch 0/938] [D loss: 0.119632] [G loss: 2.451570]\n",
      "[Epoch 80/200] [Batch 0/938] [D loss: 0.065205] [G loss: 2.938458]\n",
      "[Epoch 81/200] [Batch 0/938] [D loss: 0.096743] [G loss: 3.463799]\n",
      "[Epoch 82/200] [Batch 0/938] [D loss: 0.079241] [G loss: 2.304511]\n",
      "[Epoch 83/200] [Batch 0/938] [D loss: 0.170013] [G loss: 2.710759]\n",
      "[Epoch 84/200] [Batch 0/938] [D loss: 0.335396] [G loss: 1.089733]\n",
      "[Epoch 85/200] [Batch 0/938] [D loss: 0.056961] [G loss: 3.112450]\n",
      "[Epoch 86/200] [Batch 0/938] [D loss: 0.301521] [G loss: 2.094811]\n",
      "[Epoch 87/200] [Batch 0/938] [D loss: 0.048520] [G loss: 4.079916]\n",
      "[Epoch 88/200] [Batch 0/938] [D loss: 0.204758] [G loss: 3.657372]\n",
      "[Epoch 89/200] [Batch 0/938] [D loss: 0.233307] [G loss: 2.067572]\n",
      "[Epoch 90/200] [Batch 0/938] [D loss: 0.212678] [G loss: 5.710767]\n",
      "[Epoch 91/200] [Batch 0/938] [D loss: 0.278336] [G loss: 3.566085]\n",
      "[Epoch 92/200] [Batch 0/938] [D loss: 0.131563] [G loss: 2.294890]\n",
      "[Epoch 93/200] [Batch 0/938] [D loss: 0.221736] [G loss: 2.801384]\n",
      "[Epoch 94/200] [Batch 0/938] [D loss: 0.283954] [G loss: 1.298205]\n",
      "[Epoch 95/200] [Batch 0/938] [D loss: 0.200617] [G loss: 3.103699]\n",
      "[Epoch 96/200] [Batch 0/938] [D loss: 0.189783] [G loss: 5.537992]\n",
      "[Epoch 97/200] [Batch 0/938] [D loss: 0.109574] [G loss: 2.707323]\n",
      "[Epoch 98/200] [Batch 0/938] [D loss: 0.142200] [G loss: 2.243539]\n",
      "[Epoch 99/200] [Batch 0/938] [D loss: 0.143849] [G loss: 3.103928]\n",
      "[Epoch 100/200] [Batch 0/938] [D loss: 0.197072] [G loss: 2.095811]\n",
      "[Epoch 101/200] [Batch 0/938] [D loss: 0.161913] [G loss: 1.718210]\n",
      "[Epoch 102/200] [Batch 0/938] [D loss: 0.197630] [G loss: 2.097250]\n",
      "[Epoch 103/200] [Batch 0/938] [D loss: 0.111536] [G loss: 5.006913]\n",
      "[Epoch 104/200] [Batch 0/938] [D loss: 0.078378] [G loss: 3.684357]\n",
      "[Epoch 105/200] [Batch 0/938] [D loss: 0.068982] [G loss: 4.990858]\n",
      "[Epoch 106/200] [Batch 0/938] [D loss: 0.149669] [G loss: 2.102580]\n",
      "[Epoch 107/200] [Batch 0/938] [D loss: 0.252012] [G loss: 4.541059]\n",
      "[Epoch 108/200] [Batch 0/938] [D loss: 0.191868] [G loss: 2.655367]\n",
      "[Epoch 109/200] [Batch 0/938] [D loss: 0.243483] [G loss: 2.354600]\n",
      "[Epoch 110/200] [Batch 0/938] [D loss: 0.181750] [G loss: 2.241407]\n",
      "[Epoch 111/200] [Batch 0/938] [D loss: 0.196861] [G loss: 1.669883]\n",
      "[Epoch 112/200] [Batch 0/938] [D loss: 0.063303] [G loss: 3.548079]\n",
      "[Epoch 113/200] [Batch 0/938] [D loss: 0.166684] [G loss: 2.222865]\n",
      "[Epoch 114/200] [Batch 0/938] [D loss: 0.246326] [G loss: 2.281586]\n",
      "[Epoch 115/200] [Batch 0/938] [D loss: 0.158001] [G loss: 2.840790]\n",
      "[Epoch 116/200] [Batch 0/938] [D loss: 0.149607] [G loss: 2.762885]\n",
      "[Epoch 117/200] [Batch 0/938] [D loss: 0.223377] [G loss: 4.573430]\n",
      "[Epoch 118/200] [Batch 0/938] [D loss: 0.056337] [G loss: 3.487260]\n",
      "[Epoch 119/200] [Batch 0/938] [D loss: 0.142757] [G loss: 3.062148]\n",
      "[Epoch 120/200] [Batch 0/938] [D loss: 0.243369] [G loss: 3.806018]\n",
      "[Epoch 121/200] [Batch 0/938] [D loss: 0.304063] [G loss: 6.315752]\n",
      "[Epoch 122/200] [Batch 0/938] [D loss: 0.112284] [G loss: 2.162212]\n",
      "[Epoch 123/200] [Batch 0/938] [D loss: 0.145459] [G loss: 2.876841]\n",
      "[Epoch 124/200] [Batch 0/938] [D loss: 0.236820] [G loss: 1.791887]\n",
      "[Epoch 125/200] [Batch 0/938] [D loss: 0.113323] [G loss: 4.754183]\n",
      "[Epoch 126/200] [Batch 0/938] [D loss: 0.157833] [G loss: 3.128709]\n",
      "[Epoch 127/200] [Batch 0/938] [D loss: 0.291967] [G loss: 1.496865]\n",
      "[Epoch 128/200] [Batch 0/938] [D loss: 0.123517] [G loss: 3.441178]\n",
      "[Epoch 129/200] [Batch 0/938] [D loss: 0.221256] [G loss: 1.594026]\n",
      "[Epoch 130/200] [Batch 0/938] [D loss: 0.250522] [G loss: 4.773914]\n",
      "[Epoch 131/200] [Batch 0/938] [D loss: 0.148761] [G loss: 3.682118]\n",
      "[Epoch 132/200] [Batch 0/938] [D loss: 0.191753] [G loss: 3.670851]\n",
      "[Epoch 133/200] [Batch 0/938] [D loss: 0.458995] [G loss: 1.833665]\n",
      "[Epoch 134/200] [Batch 0/938] [D loss: 0.118080] [G loss: 3.156086]\n",
      "[Epoch 135/200] [Batch 0/938] [D loss: 0.089332] [G loss: 3.219024]\n",
      "[Epoch 136/200] [Batch 0/938] [D loss: 0.103639] [G loss: 4.721376]\n",
      "[Epoch 137/200] [Batch 0/938] [D loss: 0.117644] [G loss: 2.449107]\n",
      "[Epoch 138/200] [Batch 0/938] [D loss: 0.079189] [G loss: 3.306087]\n",
      "[Epoch 139/200] [Batch 0/938] [D loss: 0.310802] [G loss: 8.655814]\n",
      "[Epoch 140/200] [Batch 0/938] [D loss: 0.097814] [G loss: 4.077435]\n",
      "[Epoch 141/200] [Batch 0/938] [D loss: 0.306300] [G loss: 3.886711]\n",
      "[Epoch 142/200] [Batch 0/938] [D loss: 0.205481] [G loss: 4.301374]\n",
      "[Epoch 143/200] [Batch 0/938] [D loss: 0.122303] [G loss: 3.564478]\n",
      "[Epoch 144/200] [Batch 0/938] [D loss: 0.170972] [G loss: 4.300162]\n",
      "[Epoch 145/200] [Batch 0/938] [D loss: 0.194022] [G loss: 5.287113]\n",
      "[Epoch 146/200] [Batch 0/938] [D loss: 0.156886] [G loss: 2.791509]\n",
      "[Epoch 147/200] [Batch 0/938] [D loss: 0.120717] [G loss: 2.474844]\n",
      "[Epoch 148/200] [Batch 0/938] [D loss: 0.142648] [G loss: 2.584515]\n",
      "[Epoch 149/200] [Batch 0/938] [D loss: 0.062823] [G loss: 3.874342]\n",
      "[Epoch 150/200] [Batch 0/938] [D loss: 0.237205] [G loss: 5.477562]\n",
      "[Epoch 151/200] [Batch 0/938] [D loss: 0.095793] [G loss: 3.984407]\n",
      "[Epoch 152/200] [Batch 0/938] [D loss: 0.189273] [G loss: 2.112445]\n",
      "[Epoch 153/200] [Batch 0/938] [D loss: 0.155206] [G loss: 2.065143]\n",
      "[Epoch 154/200] [Batch 0/938] [D loss: 0.451854] [G loss: 6.476195]\n",
      "[Epoch 155/200] [Batch 0/938] [D loss: 0.093838] [G loss: 2.525611]\n",
      "[Epoch 156/200] [Batch 0/938] [D loss: 0.093089] [G loss: 2.990982]\n",
      "[Epoch 157/200] [Batch 0/938] [D loss: 0.152934] [G loss: 3.836830]\n",
      "[Epoch 158/200] [Batch 0/938] [D loss: 0.209822] [G loss: 1.603303]\n",
      "[Epoch 159/200] [Batch 0/938] [D loss: 0.225394] [G loss: 4.710171]\n",
      "[Epoch 160/200] [Batch 0/938] [D loss: 0.154851] [G loss: 3.279781]\n",
      "[Epoch 161/200] [Batch 0/938] [D loss: 0.243759] [G loss: 2.066335]\n",
      "[Epoch 162/200] [Batch 0/938] [D loss: 0.111837] [G loss: 3.521391]\n",
      "[Epoch 163/200] [Batch 0/938] [D loss: 0.091309] [G loss: 3.719934]\n",
      "[Epoch 164/200] [Batch 0/938] [D loss: 0.095066] [G loss: 4.535349]\n",
      "[Epoch 165/200] [Batch 0/938] [D loss: 0.080474] [G loss: 4.380523]\n",
      "[Epoch 166/200] [Batch 0/938] [D loss: 0.281274] [G loss: 1.690305]\n",
      "[Epoch 167/200] [Batch 0/938] [D loss: 0.200696] [G loss: 1.786220]\n",
      "[Epoch 168/200] [Batch 0/938] [D loss: 0.192508] [G loss: 3.958541]\n",
      "[Epoch 169/200] [Batch 0/938] [D loss: 0.068360] [G loss: 3.559227]\n",
      "[Epoch 170/200] [Batch 0/938] [D loss: 0.177015] [G loss: 4.721034]\n",
      "[Epoch 171/200] [Batch 0/938] [D loss: 0.195202] [G loss: 4.152644]\n",
      "[Epoch 172/200] [Batch 0/938] [D loss: 0.277514] [G loss: 5.619678]\n",
      "[Epoch 173/200] [Batch 0/938] [D loss: 0.158205] [G loss: 3.513690]\n",
      "[Epoch 174/200] [Batch 0/938] [D loss: 0.168330] [G loss: 3.177673]\n",
      "[Epoch 175/200] [Batch 0/938] [D loss: 0.175984] [G loss: 2.029170]\n",
      "[Epoch 176/200] [Batch 0/938] [D loss: 0.197357] [G loss: 2.869387]\n",
      "[Epoch 177/200] [Batch 0/938] [D loss: 0.265322] [G loss: 2.516891]\n",
      "[Epoch 178/200] [Batch 0/938] [D loss: 0.262352] [G loss: 1.990742]\n",
      "[Epoch 179/200] [Batch 0/938] [D loss: 0.072626] [G loss: 3.111711]\n",
      "[Epoch 180/200] [Batch 0/938] [D loss: 0.115708] [G loss: 4.311997]\n",
      "[Epoch 181/200] [Batch 0/938] [D loss: 0.289614] [G loss: 3.678600]\n",
      "[Epoch 182/200] [Batch 0/938] [D loss: 0.114279] [G loss: 5.633820]\n",
      "[Epoch 183/200] [Batch 0/938] [D loss: 0.098138] [G loss: 2.947876]\n",
      "[Epoch 184/200] [Batch 0/938] [D loss: 0.110183] [G loss: 4.175086]\n",
      "[Epoch 185/200] [Batch 0/938] [D loss: 0.114664] [G loss: 4.108710]\n",
      "[Epoch 186/200] [Batch 0/938] [D loss: 0.193376] [G loss: 3.389810]\n",
      "[Epoch 187/200] [Batch 0/938] [D loss: 0.233380] [G loss: 5.328188]\n",
      "[Epoch 188/200] [Batch 0/938] [D loss: 0.139259] [G loss: 2.865441]\n",
      "[Epoch 189/200] [Batch 0/938] [D loss: 0.100352] [G loss: 3.630192]\n",
      "[Epoch 190/200] [Batch 0/938] [D loss: 0.080106] [G loss: 4.769631]\n",
      "[Epoch 191/200] [Batch 0/938] [D loss: 0.091081] [G loss: 2.937163]\n",
      "[Epoch 192/200] [Batch 0/938] [D loss: 0.133357] [G loss: 4.079169]\n",
      "[Epoch 193/200] [Batch 0/938] [D loss: 0.119926] [G loss: 3.168817]\n",
      "[Epoch 194/200] [Batch 0/938] [D loss: 0.097900] [G loss: 5.103229]\n",
      "[Epoch 195/200] [Batch 0/938] [D loss: 0.162349] [G loss: 4.714966]\n",
      "[Epoch 196/200] [Batch 0/938] [D loss: 0.114761] [G loss: 2.651808]\n",
      "[Epoch 197/200] [Batch 0/938] [D loss: 0.118657] [G loss: 4.510206]\n",
      "[Epoch 198/200] [Batch 0/938] [D loss: 0.221968] [G loss: 6.148457]\n",
      "[Epoch 199/200] [Batch 0/938] [D loss: 0.119188] [G loss: 3.533564]\n",
      "All Done\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        if  i==0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "            )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "print('All Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
